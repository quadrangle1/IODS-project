---
title: "Chapter 4"
author: "Hanna Haltia"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 4 - Clustering and classification

The data used in this assignment is a dataset called "Boston" from the MASS package. The dataset contains variables related to housing values in suburbs of Boston, such as median value and crime rate per capita. The dataset contains 506 observations and 14 variables, all of which are either numerical or integer variables.

Let's start by loading in the data.

```{r}
# Load the required package
library(MASS)

# Activate Boston dataset
data("Boston")

# Explore the dimensions
str(Boston)
```

Next, let's take a look at the variables and their distributions.

```{r}
# Load needed packages
suppressMessages(library(ggplot2))
suppressMessages(library(GGally))
suppressMessages(library(tidyverse))

# Create a plot matrix to investigate the relationships of the variables
p <- ggpairs(Boston, lower = list(combo = wrap("facethist", bins = 20)))
p
```

The distributions of the variables are quite different, as are the ranges of the values.

Let's still look at the numerical summaries of the variables.

```{r}
# Take the summaries of the Boston variables
summary(Boston)
```

Again it is visible that some variables have very small values, whereas others have values measured in hundreds.

Next, let's standardize the data.

```{r}
# Scale data
boston_scaled <- as.data.frame(scale(Boston))

# Summaries of the scaled variables
summary(boston_scaled)
```

Now the variables have similar ranges and it's easier to compare them.

According to the instructions, let's create a new crime variable and drop the old one.

```{r}
# Summary of the scaled crime rate
summary(boston_scaled$crim)

# Create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# Create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE)

# Remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# Add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Next, we will divide the scaled data into training and test datasets.

```{r}
# Number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# Randomly choose 80% of the rows
ind <- sample(nrow(boston_scaled),  size = n * 0.8)

# Create train set
train <- boston_scaled[ind,]

# Create test set 
test <- boston_scaled[-ind,]

# Save the correct classes from test data
correct_classes <- test$crime

# Remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

Next, let's fit the linear discriminant analysis and draw the LDA biplot using the training dataset.

```{r}
# Linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# Print the lda.fit object
lda.fit

# The function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# Target classes as numeric
classes <- as.numeric(train$crime)

# Plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

Next, let's try predicting the crime rates in the test set using the LDA fit. The crime variable was already removed from the test set earlier when creating the test and training datasets.

```{r}
# Predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# Cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

The prediction is pretty good in the highest crime rate category, where there is only one observation wrongly predicted. Otherwise, the model is not very bad but also not extremely good at predicting the crime rate.

Next let's reload the Boston dataset, standardize it, and perform k-means clustering.

```{r}
# Activate Boston dataset
data("Boston")

# Standardize the data
boston_scaled <- scale(Boston)

# Euclidean distance matrix
dist_eu <- dist(boston_scaled)

# Look at the summary of the distances
summary(dist_eu)

# Set seed for reproducibility
set.seed(7)

# k-means clustering
km <- kmeans(boston_scaled, centers = 4)

# Plot the boston_scaled dataset with clusters
pairs(boston_scaled, col = km$cluster)

# Determine the number of clusters
k_max <- 10

# Calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# Visualize the results
quickplot(x = 1:k_max, y = twcss, geom = 'line')
```

Based on the first clustering attempt results visualized in the plot matrix, 4 clusters is not ideal for this data. The visualized total of within cluster sum of squares also supports 2 as the most optimal cluster number for this dataset.

Let's cluster again with k = 2.

```{r}
# k-means clustering
km <- kmeans(boston_scaled, centers = 2)

# Plot the boston_scaled dataset with clusters
pairs(boston_scaled, col = km$cluster)
```

2 clusters seems to work better than 4 clusters for this dataset.














